{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Body Data\n",
    "AMASS uses [MoSh++](https://amass.is.tue.mpg.de) pipeline to fit [SMPL+H body model](https://mano.is.tue.mpg.de/)\n",
    "to a human motion capture (mocap) session.\n",
    "[These mocaps](https://amass.is.tue.mpg.de/dataset) are from different publicly available datasets.\n",
    "A single data file in amass has the parameters to control gender, pose, shape, global translation and soft tissue dynamics\n",
    "in correspondence with the original motion capture sequence. Here we present code snippets to create a body image\n",
    "with these parameters. Since a mocap is a time sequence you visualize the \"moshed\" per frame results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the environment\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "from ipywidgets import interact, interact_manual\n",
    "from ipywidgets import IntSlider\n",
    "\n",
    "import sys, os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from human_body_prior.tools.omni_tools import copy2cpu as c2c\n",
    "from os import path as osp\n",
    "\n",
    "support_datadir = '../support_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the device to run the body model on.\n",
    "comp_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume you have downloaded the required body model and put that in body_models directory of this repository.\n",
    "For SMPL+H body model, download it from http://mano.is.tue.mpg.de/.\n",
    "You can obtain dynamic shape blendshapes, e.g. DMPLs, from http://smpl.is.tue.mpg.de/downloads.\n",
    "If you use any of these models in your research please follow their respective citation rules.\n",
    "One thing to note is that you can obtain basic SMPL+H model from their [website](http://mano.is.tue.mpg.de/),\n",
    "however, this model doesn't have DMPL parameters and has only 10 betas. Doing so will reduce the accuracy of bodies provided by AMASS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data keys available:['poses', 'gender', 'mocap_framerate', 'betas', 'marker_data', 'dmpls', 'marker_labels', 'trans']\n",
      "The subject of the mocap sequence is  male.\n"
     ]
    }
   ],
   "source": [
    "npz_bdata_path = osp.join(support_datadir, 'github_data/amass_sample.npz') # the path to body data\n",
    "bdata = np.load(npz_bdata_path)\n",
    "\n",
    "# you can set the gender manually and if it differs from data's then contact or interpenetration issues might happen\n",
    "subject_gender = bdata['gender']\n",
    "\n",
    "print('Data keys available:%s'%list(bdata.keys()))\n",
    "\n",
    "print('The subject of the mocap sequence is  {}.'.format(subject_gender))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "\n",
    "bm_path = osp.join(support_datadir, 'body_models/smplh/{}/model.npz'.format(subject_gender))\n",
    "dmpl_path = osp.join(support_datadir, 'body_models/dmpls/{}/model.npz'.format(subject_gender))\n",
    "\n",
    "num_betas = 16 # number of body parameters\n",
    "num_dmpls = 8 # number of DMPL parameters\n",
    "\n",
    "bm = BodyModel(bm_path=bm_path, num_betas=num_betas, num_dmpls=num_dmpls, path_dmpl=dmpl_path).to(comp_device)\n",
    "faces = c2c(bm.f)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided sample data also has the original mocap marker data.\n",
    "In the real AMASS dataset, only markers for the test set are included.\n",
    "For the rest of the subsets you can obtain the marker data from their respective websites.\n",
    "In the following we make PyTorch tensors for parameters controlling different part of the body model.\n",
    "\n",
    "**Please note how pose indices for different body parts work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body parameter vector shapes: \n",
      "root_orient: torch.Size([601, 3]) \n",
      "pose_body: torch.Size([601, 63]) \n",
      "pose_hand: torch.Size([601, 90]) \n",
      "trans: torch.Size([601, 3]) \n",
      "betas: torch.Size([601, 16]) \n",
      "dmpls: torch.Size([601, 8])\n",
      "time_length = 601\n"
     ]
    }
   ],
   "source": [
    "time_length = len(bdata['trans'])\n",
    "\n",
    "body_parms = {\n",
    "    'root_orient': torch.Tensor(bdata['poses'][:, :3]).to(comp_device), # controls the global root orientation\n",
    "    'pose_body': torch.Tensor(bdata['poses'][:, 3:66]).to(comp_device), # controls the body\n",
    "    'pose_hand': torch.Tensor(bdata['poses'][:, 66:]).to(comp_device), # controls the finger articulation\n",
    "    'trans': torch.Tensor(bdata['trans']).to(comp_device), # controls the global body position\n",
    "    'betas': torch.Tensor(np.repeat(bdata['betas'][:num_betas][np.newaxis], repeats=time_length, axis=0)).to(comp_device), # controls the body shape. Body shape is static\n",
    "    'dmpls': torch.Tensor(bdata['dmpls'][:, :num_dmpls]).to(comp_device) # controls soft tissue dynamics\n",
    "}\n",
    "\n",
    "print('Body parameter vector shapes: \\n{}'.format(' \\n'.join(['{}: {}'.format(k,v.shape) for k,v in body_parms.items()])))\n",
    "print('time_length = {}'.format(time_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required files for viewing out mesh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "from body_visualizer.tools.vis_tools import colors\n",
    "from body_visualizer.mesh.mesh_viewer import MeshViewer\n",
    "from body_visualizer.mesh.sphere import points_to_spheres\n",
    "from amass.tools.notebook_tools import show_image\n",
    "\n",
    "imw, imh=1600, 1600\n",
    "mv = MeshViewer(width=imw, height=imh, use_offscreen=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize betas and pose_body\n",
    "Let's see how our body looks like using the pose and body shape parameters.\n",
    "We first produce the body surface in batched mode.\n",
    "\n",
    "Now we can visualize each frame of data"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "body_pose_beta = bm(**{k:v for k,v in body_parms.items() if k in ['pose_body', 'betas']})\n",
    "\n",
    "def vis_body_pose_beta(fId = 0):\n",
    "    body_mesh = trimesh.Trimesh(vertices=c2c(body_pose_beta.v[fId]), faces=faces, vertex_colors=np.tile(colors['grey'], (6890, 1)))\n",
    "    mv.set_static_meshes([body_mesh])\n",
    "    body_image = mv.render(render_wireframe=False)\n",
    "    show_image(body_image)\n",
    "\n",
    "interact_manual(vis_body_pose_beta,fId=IntSlider(min=0, max=time_length, step=1, ontinuous_update=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=0, description='fId', max=601), Button(description='Run Interact', style…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "933fc18a67fa4f6fbb718059b8fd4577"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<function __main__.vis_body_pose_beta(fId=0)>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a male subject sitting and havig the hands open. \n",
    "Let's articulate the fingers as well:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize pose hands\n",
    "To articulate fingers we use the 66:156 pose vector elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=0, description='fId', max=601), Button(description='Run Interact', style…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "81bd5cc28c324bc6873e4515d2d0a625"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<function __main__.vis_body_pose_hand(fId=0)>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_pose_hand = bm(**{k:v for k,v in body_parms.items() if k in ['pose_body', 'betas', 'pose_hand']})\n",
    "\n",
    "def vis_body_pose_hand(fId = 0):\n",
    "    body_mesh = trimesh.Trimesh(vertices=c2c(body_pose_hand.v[fId]), faces=faces, vertex_colors=np.tile(colors['grey'], (6890, 1)))\n",
    "    mv.set_static_meshes([body_mesh])\n",
    "    body_image = mv.render(render_wireframe=False)\n",
    "    show_image(body_image)\n",
    "\n",
    "interact_manual(vis_body_pose_hand,fId=IntSlider(min=0, max=time_length, step=1, ontinuous_update=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the subject is holding something with one hand.\n",
    "\n",
    "### Visualize body joints\n",
    "\n",
    "To access joint locations of the body one can use **Jtr** attribute of the returned body.\n",
    "These can be visualized as spheres.\n",
    "Here we render the body transparently to visualize the joints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=0, description='fId', max=601), Button(description='Run Interact', style…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c048408d2f214e24a096b053c40a7695"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<function __main__.vis_body_joints(fId=0)>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vis_body_joints(fId = 0):\n",
    "    joints = c2c(body_pose_hand.Jtr[fId])\n",
    "    joints_mesh = points_to_spheres(joints, vc = colors['red'], radius=0.005)\n",
    "\n",
    "    mv.set_static_meshes([joints_mesh])\n",
    "    body_image = mv.render(render_wireframe=False)\n",
    "    show_image(body_image)\n",
    "\n",
    "interact_manual(vis_body_joints,fId=IntSlider(min=0, max=time_length, step=1, ontinuous_update=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize DMPLs\n",
    "\n",
    "You can control the soft tissue dynamics with dmpl parameters. Please have in mind, by nature of dmpl parameters being dynamic, the better appear when animation the whole sequence. Refer to full renders of the parameter sequences in our [website](https://amass.is.tue.mpg.de/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=0, description='fId', max=601), Button(description='Run Interact', style…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c0cda0931a2140bea7c25774320d5da9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<function __main__.vis_body_dmpls(fId=0)>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_dmpls = bm(**{k:v for k,v in body_parms.items() if k in ['pose_body', 'betas', 'pose_hand', 'dmpls']})\n",
    "\n",
    "def vis_body_dmpls(fId = 0):\n",
    "    body_mesh = trimesh.Trimesh(vertices=c2c(body_dmpls.v[fId]), faces=faces, vertex_colors=np.tile(colors['grey'], (6890, 1)))\n",
    "    mv.set_static_meshes([body_mesh])\n",
    "    body_image = mv.render(render_wireframe=False)\n",
    "    show_image(body_image)\n",
    "\n",
    "interact_manual(vis_body_dmpls,fId=IntSlider(min=0, max=time_length, step=1, ontinuous_update=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the global root orientation\n",
    "\n",
    "In the above examples we don't use the global translation or rotation.\n",
    "However, we can globally control the character position and orientation with **trans**, and **root_orient** parameters respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=0, description='fId', max=601), Button(description='Run Interact', style…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5168235943d44838bc632867a26f32cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<function __main__.vis_body_trans_root(fId=0)>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_trans_root = bm(**{k:v for k,v in body_parms.items() if k in ['pose_body', 'betas', 'pose_hand', 'dmpls',\n",
    "                                                                   'trans', 'root_orient']})\n",
    "\n",
    "def vis_body_trans_root(fId = 0):\n",
    "    body_mesh = trimesh.Trimesh(vertices=c2c(body_trans_root.v[fId]), faces=faces, vertex_colors=np.tile(colors['grey'], (6890, 1)))\n",
    "    mv.set_static_meshes([body_mesh])\n",
    "    body_image = mv.render(render_wireframe=False)\n",
    "    show_image(body_image)\n",
    "\n",
    "interact_manual(vis_body_trans_root,fId=IntSlider(min=0, max=time_length, step=1, ontinuous_update=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global root orientation of amass is so that if you render with MeshViewer you will always get a top view of the body.\n",
    "One can rotate the body into front view by transforming the mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=0, description='fId', max=601), Button(description='Run Interact', style…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "549a109350574e88be5f32948f2c418b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<function __main__.vis_body_transformed(fId=0)>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vis_body_transformed(fId = 0):\n",
    "    body_mesh = trimesh.Trimesh(vertices=c2c(body_trans_root.v[fId]), faces=faces, vertex_colors=np.tile(colors['grey'], (6890, 1)))\n",
    "    body_mesh.apply_transform(trimesh.transformations.rotation_matrix(-90, (0, 0, 1)))\n",
    "    body_mesh.apply_transform(trimesh.transformations.rotation_matrix(30, (1, 0, 0)))\n",
    "\n",
    "    mv.set_static_meshes([body_mesh])\n",
    "    body_image = mv.render(render_wireframe=False)\n",
    "    show_image(body_image)\n",
    "\n",
    "interact_manual(vis_body_transformed,fId=IntSlider(min=0, max=time_length, step=1, ontinuous_update=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}